
This chapter provides an overview of two machine learning experiments related to the two research questions in section \ref{sec:introduction}.
The first experiment aims to examine the effectiveness of an XGBoost model in predicting pointing scan offsets to enhance the pointing accuracy.
The primary objective of this experiment is to assess whether the proposed model can outperform the current model in terms of pointing accuracy.
The second experiment aims to investigate the effectiveness of neural networks in developing a pointing model that could replace the current linear model, which is created through linear regression.
It explores the feasibility of a more sophisticated model that can account for environmental factors in addition to the theoretical and empirical terms used in the current pointing model.

\section{Experiment 1: Pointing Correction Model} \label{sec:exp1}
This experiment aims to improve the accuracy of the existing pointing model by training XGBoost models to predict offsets obtained from pointing scans.
To accomplish this, we utilized two different datasets, which we processed using the cleaning outlined in section \ref{sec:cleaning_pt_scan}.
The difference between these datasets is that one contains the scans from all instruments, while the other only contains the scans from NFLASH230.
By training our models on these datasets, we aim to reduce the pointing offset and improve the accuracy of the pointing.
In addition, we varied the way we split the datasets for training and testing.
We considered two cases:

\begin{itemize}
    \item \textbf{Case 1:} The dataset is sorted by date and split into six equal-sized folds.
    We consider each of the folds one by one.
    For each of these folds, we use the last $1/6$th of the data as a test set and the remaining $5/6$th as training and validation.
    \item \textbf{Case 2:} The dataset is sorted by date and split into six equal-sized folds.
    We used $5/6$ of the data for training and validation and the remaining for testing.
    We repeated this process six times, using each fold for testing once.
\end{itemize}

Figure \ref{fig:datasplit_cases} illustrates the two cases.
In both cases, we trained and validated the model on $5/6$ of the data and tested on the last $1/6$.
The difference is the amount of data used for training, which can indicate whether models trained on shorter or longer periods perform better.
Using longer period, and thus more data, can help the model find complex relations.
However, a smaller period may be better for learning some relationships, as we expect less variation in a shorter period.

We also split the training and validation data such that scans from a given day only can be either in the training or validation set, not both.
When splitting the data, we used $35\%$ of the days for validation and $65\%$ for training.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.98\textwidth]{Canva/datasplits.png}
    \caption[Data split cases for pointing correction model]{This figure shows two cross-validation cases: the orange region represents the train and validation set, the red region represents the test set, and the blue region is unused for evaluation.
    In \textbf{Case 1}, the dataset is split into six equal-sized folds sorted by date.
    For the selected fold, we use the last part (colored red) for testing and the remaining part (colored orange) for training and validation.
    This process is repeated six times, once for each fold.
    In \textbf{Case 2}, the dataset is again split into six equal-sized folds sorted by date.
    However, we use one whole fold for testing this time and the remaining five for training and validation.
    This process is repeated six times, with each fold used exactly once for testing.}
    \label{fig:datasplit_cases}
\end{figure}




\subsection{Feature Selection}
We trained models using a range of features, specifically $k=[2,5,10,20,30,40,50]$ features.
For each model, we selected the $k$ features that had the greatest mutual information \eqref{eq:mutual_info} with the target value on the training and validation set.
This approach helps us identify the most important features to improve the model's performance. Selecting a subset of features can reduce the noise in the data. 
By selecting different numbers of features, we can explore the trade-off between model complexity and performance.

\subsection{Model Architecture}
We performed a hyperparameter search for each model using the parameter space in Table \ref{tab:xgb_hyperparameters_pcorr}.
The search space includes eight hyperparameters that affect the model's complexity, such as the maximum depth of the trees, the regularization strength, and the learning rate.
We used a uniform or log-uniform distribution to sample each hyperparameter within a specific range.
We evaluated $200$ different combinations of hyperparameters (for each dataset, cross-validation case, target variable, and the number of features selected) to find the optimal values for each model.
The models were validated using the MSE.

\begin{table}[H]
    \centering
    \caption[Hyperparameter search space for XGBoost pointing correction model]{This table presents a list of parameters we sampled during hyperparameter tuning for XGBoost the pointing correction model. The table includes names, sampled distributions, and corresponding ranges.}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Parameter} & \textbf{Sample Distribution} & \textbf{Range} \\ \hline
        \texttt{max\_depth} & Uniform & [$1$, $5$] \\ 
        \texttt{reg\_lambda} & Uniform & [$0$, $1$] \\ 
        \texttt{colsample\_bytree} & Uniform & [$0.5$, $1$] \\ 
        \texttt{n\_estimators} & Uniform & [$20$, $500$] \\ 
        \texttt{learning\_rate} & Log-Uniform & [$10^{-5}$, $1$] \\ 
        \texttt{subsample} & Uniform & [$0.5$, $1$] \\ 
        \texttt{gamma} & Log-Uniform & [$10^{-5}$, $1$] \\ 
        \texttt{min\_child\_weight} & Uniform & [$1$, $10$] \\ 
        \bottomrule
    \end{tabular}
    \label{tab:xgb_hyperparameters_pcorr}
\end{table}



\subsection{Model Evaluation}
To evaluate the performance of the models, we calculated the RMS on each test fold and compared it to the current RMS of the telescope on the same data.
We calculated the RMS for azimuth and elevation separately since an XGBoost model only can predict one target.
Given fold $j$ and target either azimuth or elevation, we calculate the RMS by
\begin{equation}
    RMS_{\text{target},j} = \sqrt{\frac{1}{N_j}\sum_{i=1}^{N_j} (\tilde{\delta}_{\text{target},ji} - \delta_{\text{target},ji})^2},
\end{equation}
where $\tilde{\delta}_{\text{target},ji}$ is the predicted pointing offset and $\delta_{\text{target},ji}$ is the true pointing offset for the $i$th pointing scan in fold $j$.
$N_j$ is the number of pointing scans in fold $j$. 

The measure we used for evaluating an analyzing the results is the RMS ratio between the XGBoost pointing correction model and the current model, given by
\begin{equation}\label{eq:rms_compared}
    r_{RMS,j} = \frac{RMS_{\text{target},j}}{RMS_{\text{current},j}}.
\end{equation}
This measure is useful because it compares our results to the current performance of the telescope.
If $r_{RMS,j}<1$, it indicates that the XGBoost model provides an improvement over the current performance of the telescope for a given fold.

To obtain an overall measure of the model's performance compared to the current performance of the telescope, we averaged the ratios $r_{RMS,j}$ over all six test folds
\begin{equation} \label{eq:mean_rms_compared}
    \bar{r}_{RMS} = \sum_{i=1}^6 \frac{RMS_{model,j}}{RMS_{current,j}}.
\end{equation}
This gives us an average ratio $\bar{r}_{RMS}$, which measures the expected performance of the pointing correction model.
If $\bar{r}_{RMS} < 1$, it indicates that the XGBoost model outperforms the current pointing correction method on average across all test folds.
By comparing the average ratio $\bar{r}_{RMS}$ for the two different cross-validation cases in Figure \ref{fig:datasplit_cases},
we can identify which models provide the best performance.



\section{Experiment 2: Pointing Model using Neural Networks}
This experiment uses the raw dataset containing input coordinates, $Az_{\text{input}}$ and $El_{\text{input}}$ respectively, and corresponding true observed values $Az_{\text{observed}}$ and $El_{\text{observed}}$.

The goal is to find a model $f$ such that
\begin{equation}\label{eq:pmodel_f}
    f(X) \approx (\delta_{\text{Az}}, \delta_{\text{El}}) = (Az_{\text{observed}}-Az_{\text{input}}, El_{\text{observed}}-El_{\text{input}})
\end{equation}

We split the data into a train, validation, and test set.
The last $15\%$ of the data, which we sorted by date, is used for testing.
We use the remaining $85\%$ of the data for training and validation and split this set into $20\%$ for training and $80\%$ for validation.
This results in $\sim 76\%$ and $\sim 24\%$ of the total dataset used for training and validation.

\subsection{Feature Selection}
Selecting the right features is essential in improving the pointing model's accuracy.
This model uses two types of features: geometric and harmonic terms (some of which are part of the current analytical pointing model [\eqref{eq:analytical_az},\eqref{eq:analytical_el}]) and new features extracted from the telescope's database.
For the geometric and harmonic terms, we analyzed Pearson and Spearman's rank correlation to the target values in equation \eqref{eq:pmodel_f} and picked a subset of features that showed a strong correlation. 
We did the same for the other features, except we picked out the features that showed a correlation of either type larger than $0.1$.
Tables \ref{tab:raw_data_spearmans} and \ref{tab:raw_data_pearson} list the features we extracted from the database with a correlation equal to or greater than $0.1$.
During model training, we randomly selected $n \in [2, 19] \cap \mathbb{Z}$ features from these lists and used them to train the model.
This way of choosing features does not consider complex dependencies between the features that can affect the offsets.
However, training neural networks is computationally heavy, so we had to select the features we tested carefully.

\subsection{Model Architecture}
This experiment utilized four different model architectures.
The first architecture involved feeding all input data into one or two hidden layers.
The other three architectures incorporated machine learning techniques by separating the geometric and harmonic terms of the input data from the other features and processing them using distinct architectures.
The approaches aimed to keep the current model's simplicity and performance while incorporating new features.

The following are the four different architectures:
\begin{enumerate}
    \item \textbf{Regular Neural Network:} All features are passed through the same layers, all with a nonlinear activation function.
    See Figure \ref{subfig:regular}
    \item \textbf{Neural Network with Separated Features 1:} This architecture separates the input features into two groups: geometric and harmonic features and the rest of the features.
    The geometric and harmonic features are connected directly to the linear output layer, while we pass the remaining features through layers with nonlinear activation functions.
    See Figure \ref{subfig:comb_sep1}
    \item \textbf{Neural Network with Separated Features 2:} This architecture is similar to the previous architecture,
    but we feed the geometric and harmonic features through an additional layer with a nonlinear activation function before connecting them to the output layer.
    See Figure \ref{subfig:comb_sep2}
    \item \textbf{Neural Network with Separated Features 3:} This architecture combines the previous two architectures by passing the regular features through a few hidden layers with nonlinear activation functions before concatenating them with the geometric and harmonic features.
    We then pass the combined features through a final layer before connecting them to the output layer.
    See Figure \ref{subfig:comb_sep3}
\end{enumerate}
These are visualized in Figure \ref{fig:nn_architecture}.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \input{Other figures/regular_nn.tex}
        \caption{\textbf{Regular neural network:}
        This is the standard neural network architecture without any feature separation.
        All features are connected to the same layers.}
        \label{subfig:regular}
    \end{subfigure}
    \hfill
   \begin{subfigure}[t]{0.49\textwidth}
       \centering
       \input{Other figures/combined_nn_sep1.tex}
       \caption{\textbf{Neural network with separated features 1:}
       In this architecture, the geometric and harmonic features are separated from the other features and directly connected to the output layer without any nonlinear activation function.}
       \label{subfig:comb_sep1}
\end{subfigure}
\\~\\
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \input{Other figures/combined_nn_sep2.tex}
        \caption{\textbf{Neural network with separated features 2}:
        Similar to the previous architecture, the geometric and harmonic features are separated from the other features.
        However, they are also processed by a nonlinear activation function before being connected to the output layer.}
        \label{subfig:comb_sep2}
    \end{subfigure}
    \hfill
       \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \input{Other figures/combined_nn_sep3.tex}
        \caption{\textbf{Neural network with separated features 3:}
        In this architecture, we concatenate the processed regular features to the geometric and harmonic features before being connected to the output layer.}
        \label{subfig:comb_sep3}
    \end{subfigure}
     \caption[Neural network architectures for pointing model]{The different architectures tested for the pointing model.}
     \label{fig:nn_architecture}
\end{figure}

The hyperparameters for the neural networks were randomly sampled from different distributions, as presented in Table \ref{tab:nn_hyperparameters}.
We used the Adam optimization algorithm for all models and trained $100$ networks of each architecture for $200$ epochs.
We picked the model from the epoch with the best performance on the validation set.
\begin{table}[H]
    \centering
    \caption[Hyperparameter search space for neural network pointing models]{This table presents a list of parameters we sampled during hyperparameter tuning for the base pointing model. The table includes names, the distribution we sampled from, and corresponding ranges.}
    \begin{tabular}{lcc}
    \hline
    \textbf{Name} & \textbf{Distribution Type} & \textbf{Range} \\ \hline
    hidden layers & uniform integer & [$1$,$3$] \\
    hidden layer size & uniform integer & [$20$, $120$] \\
    learning rate & uniform & [$0.001$, $0.02$] \\
    batch size & uniform integer & [$32$, $512$] \\
    activation & categorical & [gelu, tanh] \\
    loss function & categorical & [MSE \eqref{eq:mse1}, MSD \eqref{eq:msd}] \\ \hline
    \end{tabular}
    \label{tab:nn_hyperparameters}
    \end{table}

\subsection{Loss Function and Model Evaluation}
To evaluate the performance of the models, we used the root mean squared (RMS), measured in arcseconds, on the test set.
We calculate the RMS as follows:
\begin{equation}
    \text{RMS} = \sqrt{ \frac{1}{N} \sum_{i=1}^N \left( (\tilde{\delta}_{Az,i} - \delta_{Az,i})^2 + (\tilde{\delta}_{El,i} - \delta_{El,i})^2 \right)},
\end{equation}
where $\tilde{\delta}_{Az}$ and $\tilde{\delta}_{El}$ are the predicted offsets, while $\delta_{Az}$ and $\delta_{El}$ are the true values.
$N$ is the number of observations in the test set.

This RMS is used to compare the performance of the models.
It will also be compared with a benchmark linear regression model to see if a machine learning approach offers any improvements.