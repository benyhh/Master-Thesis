
This chapter of the thesis outlines the methodology used for data analysis, cleaning, transformation, and feature engineering.
Data-driven methods have the potential to learn all relations in the data, but the size of the dataset limits this.
Therefore, cleaning the data and selecting features containing information relevant to the desired output is essential.
With the system's complexity, identifying relevant features can be challenging.
To address this, we employed data-driven modeling to help identify important features while using feature engineering to incorporate
our understanding of the system and create informative features.

Cleaning data involves removing irrelevant data that could confuse the model, thereby ensuring that the model learns from the most relevant information.
Feature engineering involves incorporating domain knowledge into the model to create features that provide additional information.

We performed data analysis to decide which features to train our models.
This analysis involves understanding the relationships between the different variables in the dataset and identifying which variables
could be helpful in predicting the target variable.


\section{Cleaning Pointing Scan Data} \label{sec:cleaning_pt_scan}

When utilizing data-driven modeling for predictive purposes, ensuring that the dataset is clean and informative is crucial.
In this project, various factors may impact the quality of the data, and therefore, we implemented measures to clean the data based on our knowledge of the telescope's operation.
We employed a criteria-based approach and a machine learning classifier to remove pointing scans from the dataset.
During the removal of pointing scans, it is important to strike a balance between removing noise and retaining relevant information.
Outliers in the training data can introduce bias into machine learning models, as these data points may not accurately represent typical conditions.
Consequently, having outliers in the training data can be more damaging than removing good pointing scans.
Therefore, we have a strict approach when cleaning the data to ensure high-quality datasets for model training.


\subsection{Cleaning Criteria}
To eliminate unreliable or unusable scans, we applied criteria informed by the insights of astronomers at APEX.
The following list outlines the criteria used to filter out such scans:
\begin{itemize}
    \item Scans using the HOLO transmitter:
    These scans are aimed at a radio tower and are not realistic data for training an ML model.
    \item Scans using ZEUS2:
    These are highly experimental pointing scans and unreliable.
    \item Scans using CHAMP690: There are very few scans with this instrument.
    \item Scans in January and February of $2022$: The weather is unreliable and there are few scans in this period.
    \item Scans that are tracking tests
    \item Scans after 17.09.2022 since we only have sensory data until this point
\end{itemize}   
After this filtering, there were $5901$ out of $8862$ scans left.
        
\subsection{Pointing Scan Classifier} 
\subsubsection{Method}
In addition to cleaning the data based on the criteria above, we had to remove the outright bad pointing scans (like \ref{subfig:bad_continuous} \ref{subfig:bad_line}, \ref{subfig:bad_line}).
The scan quality is often obvious when inspecting the data visually, but it is hard to develop suitable measures to identify which scans are good or bad.
Instead, we trained a classifier to predict whether a scan is of good or bad quality.
We used an XGBoost classifier with 13 features as inputs, all of which are present in the pointing scan figures (\ref{fig:line_pointings} and \ref{fig:continueous_pointings}).
The first 12 features are the amplitudes, FWHMs, pointing offsets, and these values' uncertainties.
The last feature is the beamwidth of the telescope for the given observing frequency.\\

We had to label a dataset set by manually looking at pointing scans.
The size of the dataset set was $369$ samples with $270$ good and $99$ bad scans.
Table \ref{tab:xgb_hyperparameters_clf} shows the hyperparameters and search ranges we used when optimizing this model, along with the resulting best parameter values.
We also used \textit{scale\_pos\_weight} to consider the unbalanced classes, for which the value is the ratio of negative to positive classes (number of bad scans divided by the number of good scans).
We split the data into $80\%$ for training and the rest for validating, corresponding to $295$ and $74$ samples for training and validation, respectively.

\begin{table}[H]
    \centering
    \caption[Hyperparemter search space XGBoost pointing scan classifier]{This table presents a list of parameters we sampled during hyperparameter tuning for the pointing scan classifier.
    The table includes names, sampled distributions and corresponding ranges, and parameter values for the best model.}
    \begin{tabular}{lccc}
        \toprule
        Parameter & Sample Distribution & Range & Best Parameter Value\\ \hline
        max depth & Uniform & [$1$, $5$] & $2$\\ 
        n estimators & Uniform & [$1$, $80$] & $53$\\ 
        \bottomrule
    \end{tabular}
    \label{tab:xgb_hyperparameters_clf}
\end{table}

\subsubsection{Results}
The XGBoost classifier performed well with a $97\%$ overall accuracy on the validation set.
Figure \ref{fig:pointing_scan_clf} shows the precision-recall curve on the left and the average precision curve on the right.
From the precision-recall curve, it is clear that we can achieve close to $100\%$ precision while still having a high recall.
We select a large threshold such that the classifier removes most bad scans from the training data,
because a bad pointing scan is potentially more harmful for the model than discarding a few good scans.
The average precision curve shows an optimal threshold for maximizing the precision, which is about $80\%$.\\

We used the classifier to clean the dataset further, using prediction threshold $0.8$, we remove another $575$ scans, leaving us with
$5326$ scans for the rest of the analysis.


\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Clf/precision_recall_curve_both.pdf}
        \caption{Precision-recall curve on the test set.}
        \label{subfig:pr_curve}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
       \centering
       \includegraphics[width=1\textwidth]{Clf/mAP_curve_both.pdf}
       \caption{Average precision for different classification threshold.}
       \label{subfig:map_curve}
    \end{subfigure}
     \caption[Performance plots of XGBoost pointing scan classifier]{Precision-recall and average precision curve for the XGBoost classifier when classifying good and bad pointing scans in the test set.}
     \label{fig:pointing_scan_clf}
\end{figure}



% \\~\\
% \begin{subfigure}[t]{0.49\textwidth}
%     \centering
%     \includegraphics[width=1\textwidth]{Clf/Sage_xgb_clf_rx.pdf}
%     \caption{Average precision for different classification threshold.}
%     \label{subfig:xgb_clf_sage}
% \end{subfigure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.98\textwidth]{Clf/Sage_xgb_clf_rx.pdf}
%     \caption{SAGE values for the XGB classifier.}
%     \label{fig:xgb_clf_sage}
% \end{figure}

\section{Scan Duration Analysis}\label{sec:scan_duration_analysis}
As mentioned in the database section \ref{sec:database}, the scans' timestamps are not the accurate start time of a scan.
The tiltmeter dump files with the flag indicating whether the telescope is idle, preparing to observe, or observing,
is the only accurate data we have when the telescope performs a pointing scan.
Therefore, we need to combine the timestamp of the pointing scan with the flag in the dump files to analyze the duration of scans.

\subsection{Analysis}
First, we convert the different scan flags to numbers.
\textit{IDLE} and \textit{PREPARING} is the set $0$, and \textit{OBSERVING} is set to $1$.
Then we can subtract the previous rows from all rows, resulting in the value $1$ when the scan starts, and $-1$ when it ends.
Table \ref{tab:scan_flag_difference} shows an example of the result.


\begin{table}[H]
    \centering
    \begin{tabular}{cccc}
        \toprule
        Time & Flag & Flag Integer & $\Delta$ \\
        \midrule
        11:21:21 & IDLE & $0$ & $0$ \\
        11:21:22 & PREPARING & $0$ & $0$ \\
        11:21:23 & OBSERVING & $1$ & $1$ \\
        11:21:24 & OBSERVING & $1$ & $0$ \\
        11:21:25 & OBSERVING & $1$ & $0$ \\
        11:21:26 & IDLE & $0$ & $-1$ \\
        \bottomrule
    \end{tabular}
    \caption[Tiltmeter dump file with $\Delta$ for analysis]{This table shows the tiltmeter dump file containing the telescope state flag,
            and how we found the start ($\Delta = 1$) and end ($\Delta = -1$) of a scan.}
    \label{tab:scan_flag_difference}
\end{table}


\subsection{Algorithm}
With the scan timestamp and the observing flag from tiltmeter dumps, we used the following algorithm to obtain the start and end of pointing scans.

\begin{algorithm}[H]
    \caption{Algorithm that finds the start and end of pointing scan}
    \label{alg:scan_times}
    \begin{algorithmic}
        \Require{\\
        \begin{itemize}
            \item Pointing scan timestamps $D=\{D_1,\dots,D_n\}$
            \item Timestamps $T=\{T_1,\dots,T_m\}$ and scan flag $F=\{F_1,\dots,F_m\}$
        \end{itemize}}
        \Ensure{Start and end of pointing scans $S=\{S_i,\dots,S_n\}$ and $E=\{E_i,\dots,E_n\}$}
        \For{$i=1,\dots,m$}
            \If {$F_i=OBSERVING$}
                \State {$F_i = 1$}
            \Else
                \State {$F_i = 0$}
            \EndIf
        \EndFor
        \\
        \For{$i=1,\dots,n$}
            \State {$\hat{T} = \{T_j, \text{  if  } T_j > D_i\}_j^m$}
            \State {$\hat{F} = \{F_j, \text{  if  } T_j > D_i\}_j^m$}
            \ForAll {$t_i,f_i \text{ in } \hat{T},\hat{F}$}
                \State {$\Delta = f_i-f_{i-1}$}
                \If {$\Delta = 1$}
                    \State {$S_i = t_i$}
                \EndIf
                \If {$\Delta = -1$}
                    \State {$E_i = t_i$}
                    \State {Continue}
                \EndIf
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm} 


\subsection{Results}
By analyzing start and end timestamps for all the scans we had tiltmeter dumps for,
we see that the first \textit{OBSERVING} flag present after a scan is on average $53.9$ seconds after the scan timestamp on average, with a standard deviation of $20.5$ seconds.
Figure \ref{fig:scan_times_box} shows boxplots of this time difference for each of the instruments, which strongly indicates that assuming the starting point of a scan is $53.9$ seconds after
the timestamp is reasonable.
In the same plot, we also see that the starting time is fairly constant for the different instruments.
The right plot of Figure \ref{fig:scan_times_date} shows the time difference in seconds between the first observing flag after a scan timestamp throughout the year.
From the plot, this stays constant over time.\\

Now that we have found the starting points of the pointing scans, we can look at their duration.
The left plots in Figure \ref{fig:scan_times_date} and \ref{fig:scan_times_box} show the duration of the pointing scans for different instruments.
From these figures, it is clear that the duration of a pointing scan varies a lot.
A varying scan duration is problematic because we only have these tiltmeter dump files for $2875/8381\approx 34\%$ of the pointing scans. 
To address this issue, we collected data for feature engineering over a shorter period of time.
It is important to note that using data from after a pointing scan has ended can be inaccurate, as the telescope may start observing a different source.
When examining the scatter plot of scan durations, we observed clusters of scans around $60$-$70$ seconds, $120$-$130$ seconds, and so on.
To ensure that we collected during a pointing scan, we did not use data from more than $60$ seconds after the start of a scan.
For the scans with an exact start and end time, we used this time period instead. 


\begin{figure}[H]
    \centering
    \includegraphics[width=1.1\textwidth]{Tiltmeter plots/scan_duration_distribution_rx.pdf}
    \caption[Box plots with pointing scan duration and start time]{Box plot of the duration of scans, and the time difference between the timestamp of a scan and the actual start of it.}
    \label{fig:scan_times_box}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.1\textwidth]{Tiltmeter plots/scan_duration_distribution_date.pdf}
    \caption[Scatter plot with pointing scan duration and start time throughout 2022.]{Scatter plot of the duration of scans, and the time difference between the timestamp of a scan and the actual start of it.}
    \label{fig:scan_times_date}
\end{figure}


\section{Feature Engineering}\label{sec:feature_engineering}
Feature engineering is important to ensure that the inputs for the machine learning models are as informative as possible.
Informative features make it easier for the machine learning model to learn the relationship between features and target values.
With limited training data, this is essential.

% \textcolor{red}{Put this in feature engineering section}
% \begin{align}
%     \Delta \textit{Az}_\textit{wind} = \textit{Az}_\textit{pointing} - \textit{Az}_\textit{wind}
% \end{align}

% For the turbulence, a simple model is used
% \begin{align}
% v_\textit{wind} = \sigma_\textit{wind}^2
% \end{align}

\subsection{Median Values}
From the scan duration analysis \ref{sec:scan_duration_analysis}, we know that the duration of a scan varies.
Therefore, we use the median value during a pointing scan as a feature.

\subsection{Turbulence}
Turbulence could affect the pointing, and we used the simple model
\begin{align}
\tau_\textit{wind} = \sigma_\textit{wind}^2,
\end{align}
where the $\sigma_\textit{wind}^2$ variance in windspeed.
We calculated the variance in a period of $5$ minutes before the pointing scan.

\subsection{Position of the Sun}
Observers at the telescope report that the sun is affecting the pointing.
It is most drastically affected when the sun sets or rises, likely due to rapid temperature change leading to deformation in the telescope structure.
We also think the sun's position affects the pointing.
For instance, if the sun is shining on the left side of the telescope, it will affect the pointing differently than if it is on the right side.
Obtaining the Sun's position for the telescope's location is done using the Python module PyEphem \cite{ephem}.

Using the azimuth angle of the sun and the telescope, we can calculate the position of the sun with respect to the pointing with
\begin{equation}\label{eq:sun_az_diff}
    \Delta \textit{Az}_\odot = \textit{Az}_{\textit{t}} - \textit{Az}_\odot
\end{equation}
This will result in values outside the $[-180^\circ,180^\circ]$. An example is if $Az_\odot=179^\circ$ and $Az_t = -179^\circ$.
The calculation in equation \eqref{eq:sun_az_diff} yield $-179^\circ-179^\circ=-358^\circ$,
which corresponds to the sun being $358^\circ$ to the right of the telescope, while it ideally should be $2^\circ$ to the left.
Therefore, we adjust the values accordingly
%Write equations where \Delta Az_\odot above 180 is -= 360 and under 180 is += 360
\begin{align}
    \Delta Az_\odot = Az_\odot +360^\circ, \; \text{for} \; \Delta Az_\odot < 180^\circ\\
    \Delta Az_\odot = Az_\odot -360^\circ, \; \text{for} \; \Delta Az_\odot > 180^\circ
\end{align}

Here, the interval of the difference in azimuth is fixed to the interval $(-180^\circ,180^\circ)$,
where $0^\circ$ means the telescope is pointing towards the sun in the azimuth direction.
$\Delta \textit{Az}_\odot = 90^\circ$ corresponds to the sun being direct to the left of the pointing direction.

We also calculated the change of $\Delta Az_\odot$ in the past five minutes.
Let $\Delta Az_\odot(t)$ denote the difference between the telescope's and the Sun's azimuth given time $t$ in minutes,
then the change in the past five minutes are given by
\begin{equation}
    \Delta_5 \Delta Az_\odot=\Delta Az_\odot(t)-\Delta Az_\odot(t-5)
\end{equation}

\subsection{Time based features}
We also used time-based features to capture the time of day and year.
There are seasonal changes that affect the pointing throughout the year, and there are environmental factors throughout the day that are correlated with the time of day.
Therefore, we also used these features
\begin{itemize}
    \item \textbf{Continous month}: The number of months after 2022-01-01 00:00:00
    \item \textbf{Integer month}: The number of integer months after 2022-01-01 00:00:00 
    \item \textbf{Time of day}: The number of hours after 00:00:00 the given day
\end{itemize}

\subsection{Additional features}
There are additional features from the pointing scan data that we used as features.
The correction terms $CA$ \eqref{eq:ca} for azimuth and $IE$ \eqref{eq:ie} for elevation are used as inputs when predicting pointing offsets.
In addition, we used the type of receiver as input.
The environmental factors that cause pointing offsets might affect the different receivers differently; therefore, the model must consider this possibility.


% \begin{table}[H]
%     \centering
%     \caption{Example from the dataset of the observed pointing offsets and the corrections applied during the pointing scan.}
%     \label{tab:offset_and_correction}
%     \begin{tabular}{ccccc}
%     \toprule
%     $i$ &  $\delta_{az}$ &  $\delta_{el}$ &  $ca$ & $ie$  \\
%     \midrule
%     1 & 1.2 & 0.1 & 2.1 &  1.7 \\
%     2 &     0.0 & 0.5 & 3.3 &  1.6 \\
%     3 &    -1.1 & 0.0 & 3.3 &  1.6 \\
%     4 &     0.6 & 0.7 & 2.2 &  1.6 \\
%     5 &     0.9 & 1.4 & 2.2 &  1.6 \\
%     6 &     1.0 & 1.1 & 2.2 &  1.6 \\
%     7 &    -0.9 & 1.2 & 3.1 &  0.5 \\
%     8 &     0.5 & 1.5 & 2.2 & -0.7 \\
%     9 &    -0.3 & 0.4 & 2.2 & -0.7 \\
%     \bottomrule
%     \end{tabular}
% \end{table}


% \begin{table}[H]
%     \centering
%     \caption{Table of transformed pointing offsets and corrections according to equations \eqref{eq:ca_tilde}, \eqref{eq:ie_tilde}, \eqref{eq:off_az_tilde}, and \eqref{eq:off_el_tilde}.}
%     \label{tab:tranform_offsets}
% \begin{tabular}{ccccc}
% \toprule
% $i$ & $\Tilde{\delta}_{az}$ &  $\Tilde{\delta}_{el}$ &  $\Tilde{ca}$ &  $\Tilde{ie}$ \\
% \midrule
% 0 &       1.2 &       0.1 &       2.1 &       1.7 \\
% 1 &       0.0 &       0.5 &       3.3 &       1.6 \\
% 2 &      -1.1 &      -0.5 &       3.3 &       1.1 \\
% 3 &       0.7 &       0.7 &       2.2 &       1.6 \\
% 4 &       0.2 &       0.7 &       2.8 &       0.9 \\
% 5 &       0.1 &      -0.3 &       3.1 &       0.2 \\
% 6 &      -1.0 &       1.3 &       3.2 &       0.5 \\
% 7 &       0.5 &       1.4 &       2.2 &      -0.7 \\
% 8 &      -0.8 &      -1.1 &       2.7 &      -2.2 \\
% \bottomrule
% \end{tabular}
% \end{table}

