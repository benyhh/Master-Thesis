\section{Experiment 1: Base Pointing Model}
 

\section{Experiment 2: Pointing Correction Model}
There are many interesting things to discuss about the pointing correction model.
First of all, we considered two different cases of training and testing the models, case $1$ and $2$.
In case $1$, we use smaller amounts of data for training and testing each model, and in case $2$, we used more data for trainig and testing.
The models using smaller amounts of data for training a model, and then testing the performance on some scans coming after the training period, 
did not offer any improvement what so ever. This shows that the models perhaps require more data in order to learn meaningful relationships in the data.
The models did, however, show great results on the validation set, with validation RMS compared ratio in the range $0.8$-$0.9$ \textcolor{red}{Include table in appendix}.
This shows that the models are able to learn something from the data, but that there is not enough data to generalize well onto new data.
There is also a clear difference in the complexity for Azimuth and Elevation models, showing that finding relations in the dataset
that affect elevation offsets are easier to learn than that of azimuth. 
In the context of the first research question, we wanted to see if a machine learning model can use data to learn what causes the pointing offsets.
The results show that it is possible to slighly improve the pointing accuracy. 
For the model predicting offsets for all instruments, the performance on the test sets are improved by $2\%$ for azimuth and $4.5\%$ for elevation.
That is if we choose the number of features to train the models for azimuth and elevation based on the performance on the test set.
Doing this, however, lead to a bias in the results, leading to the conclusion that we may not be able to expect a slight improvement by implementing this strategy in practise.
However, if we choose the approach of selecting the number of feature that yield the best performance on the validation set, and then use that model to correct the pointing for the next period in time,
we have a less biased result. 
By choosing this strategy, we see from Table \ref{tab:pcorr_min_val_days_04} no increased performance for azimuth and a $5.0\%$ decreased pointing offset in elevation.
This is still not a performance we could expect to see in practise,
given that these results are obtained from by having a training/validation set that falls both before and after the test set in time.
The only realistic result that show an actual expected performance, is the results of the model using a test set that is after the train/validation set in time, which is the last fold.
From Table \ref{tab:minval_fold5}, we see a $7.0\%$ and $9.4\%$ reduced RMS for the NFLASH230 model, which is a completely unbiased result.
The features used to train the models are picked through the largest mutual information with the target value on the train/validation, and the model is tested on a time period after the training/validaiton period.
These results compared to the model predicting the pointing offsets for all instruments tells us that the amount of training data is an important apect in this problem.
There are multiple ways of increasing the performance further, the first one being the obvious more training data. 
The second way to do it is by engineering better features.
We spent a lot of time figuring out what features to construct, but the analysis thereafter could have been more thorough.
After making all the features, we performed correlion analysis on the whole dataset, which proved to not contain much correlation at all.
However, we did not perform the same analysis on shorter time periods, which could have been more fruitful.
Doing this could have helped us grain more insights in what could bring us better predictions.
This might not have impacted the performance, since the features were picked based on the mutual information on the time period of the training/validation data.
There could also have been better ways of engineering some of the features, for instance temperature features.
We considered the different temperature measurements located at various locations,
but what we did not do is constructing features based on the difference in temperature at the different locations.
Making these features, and then performing analysis on shorter periods could have provided much more insightfull features.\\
This discussion answers the first research question






\begin{itemize}
    \item Not generalizing very well, difference between validation and test
    \item Not complex models ofr azimuth, sign of not having enough data for omplex models
    \item Perhaps better feature engineering
    \item Perhaps other tests
    \item Can be used now and we would expect slight improvement, atleast for nflash230
    \item Future work is testing if it can reduce number of pointing scans and have similar performance
    \item Better feature engineering and analysis. Plot over time, correlations in shorter time periods.
    \item Need to present more of the transformed variables. could actually compare them with untransformed since the model can know all thei nformation even though correction isnt made.
\end{itemize}


Present the results in two way.

Main results are NFLASH model with Case 1 and 2. This identifies that we need more data than provided in the period of case 1.
Show the results with number of features and mean rms ratio, to show an estimate of which models yield the best performance.
Then go onto sayin that this is not a good estimate of performance, as the results are biased. Therefore, we use the min val on last fold model,
and present that as a result on unbiased performance.
Then present the same results for all instrument models, to show that we need more data to generalize.


In results sectyion, present first nflash with num features, then all instruments. Then nflash min val, then all instruments min val.0*** vfdos2*-
r||